{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation and grid search\n",
    "\n",
    "## What will you learn in this course? 🧐🧐\n",
    "\n",
    "This course will teach you about a model evaluation technique called cross-validation, it will be very useful to get a more precise idea of how well your model performs on unknown data. It will also help you pick the best possible hyper-parameters for your model (e.g. the regularization strength)\n",
    "\n",
    "* How to protect yourself from overfitting\n",
    "* Diagnosing under/over-fitting\n",
    "* Hyperparameter optimization\n",
    "     * Example : tuning the regularization strength\n",
    "     * Cross-validated grid search\n",
    "     * Remark about computation time\n",
    "         * Example 1 : tuning one hyperparameter\n",
    "         * Example 2 : tuning several hyperparameters at the same time\n",
    "\n",
    "## How to protect yourself from overfitting ⛔️\n",
    "\n",
    "Under-fitting situations occur in practice, due to a lack of relevant data or can usualy be solved by using a more complex model. The most common enemy of data scientists however, is overfitting, because it gives the illusion of performance, but is actually a trap!\n",
    "\n",
    "A simple and effective way to avoid this trap is to practice k-fold cross validation (other types of cross-validation techniques exist but we will focus on this one during the whole duration of the bootcamp). It is a process that consists in choosing an integer k (often 3 or 10 is chosen by default depending on the librairy used), and randomly splitting the observations into k groups of equal size. Then the following method is repeated k times:\n",
    "\n",
    "* We isolate one group i among the k groups, which we call **validation set**, and we gather the 9 others, which we call **training set**.\n",
    "* Estimate the chosen model using the **training set**.\n",
    "* We calculate the error committed by model i on the **validation set** (group i).\n",
    "\n",
    "The comparison of the train error and the validation error allows us to understand the real explanatory power of a model, because it quantifies the performance of the model on different sets of unknown data compared to its performance on known data.\n",
    "\n",
    "As opposed to training the model on the train set and evaluate it using the test set, which only allows us to test the model's performance in one specific arrangement of the data, we now are able to measure the model's performance in a collection of randomly generated contexts using the data and figuring out how it performs on average. Cross validation gives information on how the model performs, and how stable or unstable its performance is depending on the data distribution.\n",
    "\n",
    "![overfitting](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/cross_validation.png)\n",
    "\n",
    "\n",
    "The figure above illustrates the principle of k-fold cross-validation. Each iteration produces results in terms of validation error and training error that are used to evaluate the model. These errors are usually calculated based on the cost function chosen to optimize the model, or simply on the average of the errors squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing under/over-fitting 🩺🩺\n",
    "To detect underfitting or overfitting, the train/test (or train/validation) scores can be compared to each other :\n",
    "* If $score(train) \\sim score(test) \\sim 0$ : the model is underfitting\n",
    "* If $score(train) >> score(test)$ : the model is overfitting\n",
    "* If $score(train) \\sim score(test) >> 0$ : the model is just right !\n",
    "\n",
    "> 👋 $>>$ means *lot more than*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization 🔧🔧\n",
    "\n",
    "In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. \n",
    "\n",
    "A hyperparameter is a parameter whose value is used to control the learning process, but is not directly used to compute the model's function. Hyperparameters are not tuned at the learning step, they have to be determined beforehand.  By contrast, the values of other parameters (typically, the coefficients of the model's function) are learned at the training step.\n",
    "\n",
    "### Example : tuning the regularization strength\n",
    "An example of model hyperparameter is the regularization strength (noted either $\\lambda$ or $\\alpha$, depending on the convention) in regularized linear regression models. The value of $\\alpha$ has to be tuned to find the right model complexity and get performances that are \"just right\" between underfitting and overfitting.\n",
    "\n",
    "### Cross-validated grid search\n",
    "The traditional way of performing hyperparameter optimization is grid search, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set.\n",
    "\n",
    "The cross-validated grid search algorithm can be described by the following steps :\n",
    "* Determine the list of values of the hyperparameters to be tested (for example : $\\alpha = [0.1, 0.5, 1.0]$)\n",
    "* For each combination of hyperparameters, perform k-fold CV to estimate the generalized score achieved with each hyperparameters values\n",
    "* Pick the set of hyperparameters that define the model with the highest cross-validated score\n",
    "* Re-train the model with this set of hyperparameters, on the whole training set\n",
    "\n",
    "![grid_search](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/grid_search.png)\n",
    "\n",
    "### Remark about computation time\n",
    "Cross-validated grid search can be very time-consuming because the model training is repeated a high number of times ! Actually, for k-fold cross-validation, the number of model trainings n can be expressed as :\n",
    "\n",
    "$$\n",
    "n = k \\times n_{comb}\n",
    "$$\n",
    "\n",
    "where $n_{comb}$ is the total number of combinations of hyperparameters values to be tested.\n",
    "\n",
    "#### Example 1 : tuning one hyperparameter\n",
    "Let's imagine we want to tune the regularization strength $\\alpha$ and test 3 different values : $\\alpha = [0.1, 0.5, 1.0]$. We would like to use 5-fold cross-validation.\n",
    "\n",
    "Then, the total number of trainings in the cross-validated grid search will be :\n",
    "\n",
    "$$\n",
    "n = 5 \\times 3 = 15\n",
    "$$\n",
    "\n",
    "#### Example 2 : tuning several hyperparameters at the same time\n",
    "It is very common that we actually tune several hyperparameters at the same time. Let's imagine we want to tune three hyperparameters (respectively named $\\alpha$, $\\beta$ and $\\gamma$) and we want to try the following values:\n",
    "* $\\alpha = [0.1, 0.5, 1.0]$ \n",
    "* $\\beta = [1, 2, 3, 4]$\n",
    "* $\\gamma = [-1, 1]$\n",
    "\n",
    "The numbers of values to be tested for each hyperparameter are respectively :\n",
    "* $n_\\alpha = 3$\n",
    "* $n_\\beta = 4$\n",
    "* $n_\\gamma = 2$\n",
    "\n",
    "Then, the total number of combinations to be tested is :\n",
    "\n",
    "$$\n",
    "n_{comb} = 3 \\times 4 \\times 2 = 24\n",
    "$$\n",
    "\n",
    "If we want to perform 5-fold cross-validation, the total number of trainings in the cross-validated grid search will be :\n",
    "\n",
    "$$\n",
    "n = 5 \\times 24 = 120\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources 📚📚\n",
    "\n",
    "* [Cross Validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "\n",
    "* [Kfold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)\n",
    "\n",
    "* [Exhaustive Grid Search](https://scikit-learn.org/stable/modules/grid_search.html#exhaustive-grid-search)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
