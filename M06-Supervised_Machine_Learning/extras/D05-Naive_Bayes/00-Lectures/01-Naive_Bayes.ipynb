{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning - Naive Bayes\n",
    "\n",
    "<!-- TOC START min:2 max:4 link:true asterisk:true update:true -->\n",
    "* [What you will learn in this class](#what-you-will-learn-in-this-class)\n",
    "* [Bayes theorem](#bayes-theorem)\n",
    "* [Naive Bayes](#naive-bayes)\n",
    "  * [Case where $X_i$ is qualitative](#case-where--is-qualitative)\n",
    "  * [Case where $X_i$ is quantitative](#case-where--is-quantitative)\n",
    "* [General remarks](#general-remarks)\n",
    "<!-- TOC END -->\n",
    "\n",
    "\n",
    "\n",
    "## What you will learn in this class\n",
    "\n",
    "This course is dedicated to the teaching of the so-called Naive Bayes model. It is a model that relies on the explanatory variables being independent from each other, a very strong hypothesis that is very rarely verified in practice. Nevertheless, this model can be very useful and provides a good view of the influence of each variable on the target variable within the model.\n",
    "\n",
    "In statistics, Bayesian naive classifications belong to a family of probabilistic classifications based on Bayes' theorem.\n",
    "\n",
    "## Conditional probability\n",
    "Let's start this lecture by introducing the concept of conditionnal probabilities. Think of the following example :\n",
    "\n",
    "You are flipping two coins one after the other, these coins are balanced. It may not seem so but we have already used many probabilistic concepts here :\n",
    "* Flipping a coin is defining a *random variable*, a random variable is a set of values, in our case *tail* and *head*, to which probabilities are associated (or a probability distribution in case the value space is continuous or more exactly non countable). Here the coins are balanced which means $P(tail) = P(head) = 0.5$\n",
    "* Flipping two coins one after the other is introducing two additional concepts : we just defined what is called a random experiment (the fact of throwing two coins, examining the outcome of two random variables), plus the concept of independence, the two coins are not related to each other in any, so the outcome of one flip does not have any influence on the other flip and vice versa.\n",
    "\n",
    "Let us now define two *events* :\n",
    "* Event $A$ is the first coin is *tail*\n",
    "* Event $B$ is both coins are *tail*\n",
    "\n",
    "We can now calculate the probabilities for these two events to happen :\n",
    "\n",
    "$P(A)=P(coin_1 = tail)=0.5$\n",
    "\n",
    "$P(B)=P(coin_1 = tail,\\, coin_2 = tail)=P(coin_1 = tail)\\times P(coin_2 = tail) = 0.5\\times 0.5 = 0.25$\n",
    "\n",
    "To calculate the probability of event B, we are actually calculating the *joint* probability of coin 1 and coin 2 both being tail during the same random experiment, for two independent events, the joint probability equals the product of each individual probability.\n",
    "\n",
    "Let us now indtroduce the concept of conditional probabilities. The conditional probability of $B$ given $A$ is the probability that event $B$ is realised given the fact that event $A$ already happened. It is written as follows :\n",
    "\n",
    "$ \\begin{align*}\n",
    "P(B|A) &= P(coin_1 = tail,\\, coin_2 = tail|coin_1 = tail) \\\\\n",
    "       &= P(coin_1 = tail | coin_1 = tail)\\times P(coin_2 = tail | coin_1 = tail) \\\\\n",
    "       &= 1 \\times P(coin_2 = tail) \\\\\n",
    "       &= 1 \\times 0.5 \\\\\n",
    "       &= 0.5\n",
    "\\end{align*} $\n",
    "\n",
    "Here to go from line 1 to line 2 we use the fact that coin 1 and coin 2 are independent so the joint conditional probability becomes the product of both conditional probabilities. The probability of coin 1 being tail given coin 1 is tail is ... 1 ! We already know that coin 1 is tail here. Then because the result of coin 1 does not influence coin 2 the second conditional probability is that of coin 2 being tail, therefore 0.5. The conditional probability of $B$ given $A$ is then 0.5, when probability of $B$ is normally 0.25 !\n",
    "\n",
    "Everything else in the lecture will be based on these probability rules.\n",
    "\n",
    "## Bayes theorem\n",
    "\n",
    "Bayes theorem corresponds to the following statement:\n",
    "\n",
    "Let $A$ and $B$ be two random variables, then the following equality is verified:\n",
    "\n",
    "### $P(A|B)=\\frac{P(B|A)\\cdot{P(A)}}{P(B)}$\n",
    "\n",
    "\n",
    "The conditional probability of $A$ knowing $B$, $P(A|B)$ is equal to the product of the conditional probability of $B$ knowing $A$, $P(B|A)$, and the probability of $A$ divided by the probability of $B$.\n",
    "\n",
    "\n",
    "\n",
    "## Naive Bayes\n",
    "\n",
    "We consider the situation where we have $Y$ the qualitative target variable (so it is a classification problem) that we are trying to predict, and a collection of explanatory variables $X=(X_1,X_2,...,X_p)$. The problem is to estimate for each observation the law $P(Y/X)$, which gives the probability for $Y$ to take each of its possible values, knowing the values of $X$ for this observation.\n",
    "\n",
    "Bayes's theorem intervenes here and gives us the following writing:\n",
    "\n",
    "\n",
    "### $P(Y|X)=\\frac{P(X|Y)\\cdot{P(Y)}}{P(X)}$\n",
    "\n",
    "\n",
    "\n",
    "The denominator does not involve $Y$ and therefore has no influence on the model results, we will only be interested in the numerator which can be recursively decomposed using the properties of conditional probabilities. The property we will use here is the fact that :\n",
    "$P(A,B)=P(A|B)P(B)$\n",
    "We then get the following :\n",
    "\n",
    "### $P(X|Y)\\cdot{P(Y)}=P(X_1,X_2,...,X_p,Y)$\n",
    "\n",
    "### $P(X|Y)\\cdot{P(Y)}=P(X_1|X_2,...,X_p,Y)\\cdot{P(X_2,...,X_p,Y)}$\n",
    "\n",
    "### $P(X|Y)\\cdot{P(Y)}=P(X_1|X_2,...,X_p,Y)\\cdot{P(X_2|X_3,...,X_p,Y)}\\cdot{P(X_3,...,X_p,Y)}$\n",
    "\n",
    "### $P(X|Y)\\cdot{P(Y)}=P(X_1|X_2,...,X_p,Y)\\cdot{P(X_2|X_3,...,X_p,Y)}\\cdot{P(X_3|X_4,...,X_p,Y)...P(X_p|Y)\\cdot{P(Y)}}$\n",
    "\n",
    "This is where we need the fundamental and naive assumption that allows us to build our estimates : *all the explanatory variables must be independent*, because then we have for all $i$ between $1$ and $p$:\n",
    "\n",
    "### $P(X_i|X_{i+1},...,X_p,Y)=P(X_i|Y)$\n",
    "\n",
    "\n",
    "In fact, you get:\n",
    "\n",
    "### $P(Y|X)=\\frac{P(Y)P(X_1|Y)P(X_2|Y)...P(X_p|Y)}{P(X)}$\n",
    "\n",
    "\n",
    "\n",
    "Which is very simple to calculate since we just need to estimate for each value of $Y$ the distribution of $X_i$.\n",
    "\n",
    "\n",
    "\n",
    "### Case where $X_i$ is qualitative\n",
    "\n",
    "In the case where $X_i$ is a qualitative explanatory variable that takes the modalities $x_{i1},...,x_{iq}$ then we can write :\n",
    "\n",
    "### $\\hat{P}(X_i=x_{ik}|Y=y)=\\frac{Card(X_i=x_{ik}, Y=y)}{Card(Y=y)}$\n",
    "\n",
    "\n",
    "We estimate the probability that $X_i$ takes the modality $x_{ik}$ knowing that $Y = y$ as the proportion of observations where $X_i=x_{ik}$ among all observations where $Y = y$.\n",
    "This is very easy to calculate and can be very accurate with very little data.\n",
    "\n",
    "\n",
    "### Case where $X_i$ is quantitative\n",
    "\n",
    "For cases where $X_i$ is quantitative in general, we go back to the qualitative case by cutting the range of values of $X_i$ into $K$ pieces indexed by $k\\in{[[1,K]]}$ and delimited by the values of $-\\infty=\\alpha_0,\\alpha_1,...,\\alpha_{k-1},+\\infty=\\alpha_k$ and the probability law becomes :\n",
    "\n",
    "### $\\hat{P}(X_i=x_{ik}\\in[\\alpha_j, \\alpha_{j+1}]|Y=y)=\\frac{Card(X_i\\in[\\alpha_ j,\\alpha_{j+1}],Y=y)}{Card(Y=y)}$\n",
    "\n",
    "\n",
    "I.e. the proportion of observations for which the value of $X_i$ belongs to the interval $[\\alpha_j,\\alpha_{j+1}]]$ among all observations for which $Y = y$. This technique is called discretization of a continuous variable.\n",
    "\n",
    "Another way to estimate the law of $X_i$ knowing $Y$ is to make the hypothesis that $P(X_i|Y)$ follows a normal law whose parameters $\\mu_i$ (mean) and $\\sigma_i$ (standard deviation) are estimated thanks to the data available on $X_i$. Under the assumption of normality, $P(X_i|Y)$follows a normal law of parameters :\n",
    "\n",
    "\n",
    "### $\\mu_{iy}=\\frac{1}{N_y}\\sum_{j=1}^{N_y}x_{ij}$ \n",
    "\n",
    "\n",
    "Which is the average value of $X_i$ among the $N_y$ individuals for whom $Y = y$. Similarly, we calculate the variance of the normal law:\n",
    "\n",
    "\n",
    "### $\\sigma_{iy}^2=\\frac{1}{N_y-1}\\sum_{j=1}^{N_y}(x_{ij}-\\mu_{iy})^2$\n",
    "\n",
    "\n",
    "\n",
    "The variance estimator of $X_i$ among individuals for whom ***Y = y***. Once this estimation is done, we get:\n",
    "\n",
    "\n",
    "### $\\hat{P}(X_i=x_{ik}|Y=y)=\\frac{1}{\\sqrt{2\\pi\\sigma_{iy}}}exp(\\frac{-(x_{ik}-\\mu_{iy})^2}{2\\sigma_{iy}^2})$\n",
    "\n",
    "\n",
    "Once all the conditional probabilities have been calculated, we obtain for each observation and for each modality of $Y$ a probability which determines our classification. Each observation will be classified in the modality of $Y$ that is the most probable given the values of the explanatory variables $X$.\n",
    "\n",
    "## General remarks\n",
    "\n",
    "An advantage of the naive Bayes model is that it allows us to avoid making assumptions about the distribution laws of the explanatory variables if we transform them into qualitative variables. However, it is very rare that the fundamental hypothesis of the independence of the explanatory variables is verified in practice.\n",
    "\n",
    "Naive Bayesian models can be aggregated in the same way as random trees, which generally gives much more stable results and also better respects the hypothesis of independence of the explanatory variables if, as we have seen in the case of random forest, only part of the explanatory variables are used to build each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}